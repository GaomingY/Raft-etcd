# Raft-etcd
本项目主要介绍Raft分布式一致性协议，并且详细讲解了分布式系统的设计理念和面临的各种问题。最后，通过etcd的源码来讲解一下Raft的落地实践

# 分布式系统
若干个单机系统通过网络连接起来就是分布式系统，它本质上是一堆电脑，每个电脑都有自己独有的CPU和硬盘，然后这堆电脑通过网络连接起来，互相之间可以通信。
(当然，现在的存算分离技术将计算资源和存储资源分离开了，也就是说有些电脑只负责计算，没有硬盘存储数据，有些电脑只负责存储数据，不具备计算数据的能力)

在之后的讲解中，我将电脑这个概念抽象成节点。
分布式系统又可以分成横向分布式系统和纵向分布式系统。分布式系统相较于单机系统最大的两个优势就是数据备份和负载均衡，这两个特点支撑了分布式系统提供规模更大，质量更高的数据服务

横向分布式系统不对单个节点性能做过多的提升，它旨在通过增多系统中节点的数量来提高系统的性能
纵向分布式系统也叫微服务架构，它主要是纵向将每个节点需要提供的服务进一步切分，比如将完整服务切分成用户模块，支付模块等等，通过提高单机性能来提高整体的性能
本项目的主角——Raft，主要就是为了解决横向分布式的一系列问题，当节点的数量不断增多，如何保证节点之间的秩序，保证在不同节点之间重复备份的数据的一致性，这就是它要考虑的问题

# CAP理论
C指的是consistency，一致性；A指的是availability，可用性；P指的是Partition tolerance，分区容错性
一致性是指数据一定是正确的，同一个数据的不同备份应该是相同的，这样用户在读数据时才不会读到错误的数据，或是过时的数据
可用性指的是分布式系统服务的高质量，分布式系统应该为client提供快速响应的数据和计算服务
分区容错性指的是在分布式系统中某些节点崩溃时，整个系统仍然能够继续运行

在一个分布式系统中，无法同时满足三者，最多只能同时满足两个。例如，要满足C，就需要某一个节点在接受到用户的写数据请求后，首先保证各个节点之间的数据更新到最新版本，然后才能给用户反馈数据改写已完成
这就会造成一个写操作的时延非常高。

如果要保证A，那么节点在改变本机数据后就会第一时间反馈给用户操作成功，然后才会去同步各个节点的数据。用户是用爽了，请求很快就完成了，然后就发了疯似地改数据，但系统要疲于更新各个节点中的数据，这就会出现因为网络延迟而导致的数据不一致问题。

因此，现代分布式系统主要有两个流派：CP流派和AP流派，在保证一个方面的同时尽可能满足另一个方面，而不是直接舍弃另一个方面

因此，所有的分布式一致性协议，包括Raft，它们要做的就是合理地取舍C和A，从而提高系统的整体下限

# 多数派原则
对于任何一项提议，这个提议可以是leader选举，也可以是分布式系统中的任何一项事务，只要有大于半数以上（不包括一半）的节点赞同，这个提议就可以通过并执行
因此，在设计系统时，节点数量尽量定为奇数，这样就可以提高容错，允许更多的节点因出现错误而无法投票（大多数情况，提议的反对票是因为节点投不出来票的原因）
半数通过的原则可以提高系统的可用性A，因为任何一项系统内发生的事务都不需要全员通过的严苛要求

# 一主多从，读写分离
一主多从很好理解，系统内同一时间只有一个合法leader，剩下的都是follower，leader负责集群内各项事务的决断，而follower就只负责干活，但follower遵循多数派原则，具有投票权，当多数follwer赞同时，它们甚至可以推翻leader
读写分离的意思就是读操作可以由集群内任意一个节点来提供服务，可以是leader，也可以是follower；而写操作只能由leader处理，并向follwer同步，即使follower收到了写请求也要发送给leader处理

# 状态机和预写日志
状态机就是存储数据的容器，可以是数据库，也可以是其他的存储介质，它的特点是只能保存最终状态，不负责记录中间状态，因此，只有状态机是远远不够的，因为如果出现错误了连回滚的机会都没有
因此，预写日志就出现了，它本质上是一个由日志记录组成的数组，每一次操作都对应这日志中的一条记录，而这个记录也拥有一个索引值index，但每一个term结束后，新任leader可能会使索引归零，因此，需要<term, index>二元组才能定位唯一日志记录
预写日志之所以有预写二字，是因为在真正的数据操作之前，它就被记录下来了，之后数据操作才会被真正执行

# 两阶段提交
当写请求到达leader后，它会首先发出一个提议，然后集群中的follower进行投票，通过多数派原则来决定这次写请求是否应该被执行，这就是第一阶段——提议阶段
follower在收到leader发送来的提议之后，会检验自己是否能够执行同步（添加预写日志），当多数节点向leader发送回提交请求后，leader就会正式提交这个请求
在随后的心跳交互过程中，follower也会收到这个提议被通过，也就是提交的结果，并在自己的预写日志中也提交这笔请求
最后一步就是将提交的请求应用到状态机中，但根据对一致性的要求不同，它的实现也会不同，如果要求最终一致性，就要同步应用状态机和返回客户端ack，如果要求立即一致性就需要先应用状态机，之后再返回ack

# 领导者选举
领导者选举需要解决的最主要的问题就是如何在上一任leader瘫痪之后第一时间感知到，然后再重新选举出一个合格的leader?
感知leader靠的是心跳机制，leader需要向所有的follower广播心跳，告知其自己还健在，当follower的心跳检测计时器超时之后还没有收到leader的心跳，则认为leader已死，自己就会称为candidate发起竞选
竞选机制就是各个节点进行拉票，当自己收获的全票超过半数以上，它就会成为新的leader
当然，follower之后认同那些具备更新WAL的candidate，并把自己的票投出去

# 跟随者
follower来到Raft只做三件事：

(1) 收到一个竞选人的拉票信息，然后做出决断，赞同或反对

(2) 收到leader的同步日志信息，然后更新自己的WAL

(3) 收到Leader的心跳信息，然后重置自己的心跳检测计时器

# 候选者
候选者只是领导者和跟随者的一个中间状态，节点不会在候选者的状态中一直停留，最终它是要竞选成功成为领导者，或竞选失败成为跟随着
一个节点成为候选者的第一步就是给自己投一票，然后向各个follower拉票
当某个候选者获得半数以上的选票时，这个候选者会成为领导者，其他所有候选者退化成追随者
当某一任期没有成功选举出领导者之后，每个候选者会递增自己的任期然后再次竞选

# 写请求流程
一个写请求的处理要经历三个阶段：proposal，commit，applied

proposal阶段是leader征求各个follower同意的阶段，当半数以上节点同意这个写请求后，它就会从proposal变成commit阶段，当然，proposal有可能失败，原因是leader的WAL和follower的WAL不一致，这个不一致又可以分成leader滞后和follower滞后，leader滞后就需要重新选举leader，follower滞后就需要leader和它进行WAL同步，两种滞后都会导致该节点反对此次写操作

当写操作从proposal阶段进入到commit阶段之后，就代表此次写操作已经被系统认可，迟早都会写入到系统的状态机中

最后一个阶段就是applied阶段，代表此次写操作的内容已经成功写入状态机

# 读请求流程
用户的读数据请求会被集群中的任意节点处理，由于预写日志+两阶段提交+多数派原则，状态机的数据具有最终一致性，但不具备即时一致性
也就是用户写进去的数据要过一会儿才能达到整个系统都一致的状态，如果用户立马就要读这个数据就不一定读到最新的数据
解决即时一致性的方法就是在客户端写入一个数据后，leader将预写日志中已经应用到状态机的数据的最大索引值appliedIndex返回给客户端，在客户端想要读这个数据时会带上这个索引值
当给这个客户服务的节点发现自己预写日志中的appliedIndex小于这个客户端带来的这个索引值时，说明自己的状态机的数据已经过时（相较于此时服务的客户），自己已经没有资格继续服务这个客户，就会转交给其他的节点来服务

另外一种方法就是强制读主，这样一来，服务客户端的节点就只有leader一个了，其他所有的follower都只是数据备份的节点罢了，这就会给leader带来很大的网络负载
同时读数据时还要加上验证leader的步骤，来确保这个leader是当前真正的leader















